<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Q-Learning Navigation | RL Research Project</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

ƒ        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.7;
            color: #333;
            background: #fff;
        }

        /* Navigation */
        nav {
            background: #fff;
            border-bottom: 1px solid #e0e0e0;
            padding: 1rem 0;
            position: sticky;
            top: 0;
            z-index: 100;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }

        nav .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 2rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        nav .logo {
            font-size: 1.2rem;
            font-weight: 600;
            color: #1a73e8;
        }

        nav ul {
            list-style: none;
            display: flex;
            gap: 2rem;
        }

        nav a {
            color: #333;
            text-decoration: none;
            font-size: 0.95rem;
            transition: color 0.2s;
        }

        nav a:hover {
            color: #1a73e8;
        }

        /* Header Section */
        .header {
            max-width: 1200px;
            margin: 0 auto;
            padding: 4rem 2rem 3rem;
        }

        .header-content {
            display: grid;
            grid-template-columns: 200px 1fr;
            gap: 3rem;
            align-items: start;
        }

        .profile-image {
            width: 200px;
            height: 200px;
            border-radius: 50%;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-size: 3rem;
            font-weight: 300;
        }

        .header-text h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
            font-weight: 400;
        }

        .header-text .subtitle {
            font-size: 1.2rem;
            color: #666;
            margin-bottom: 0.3rem;
        }

        .header-text .affiliation {
            font-size: 1.1rem;
            color: #1a73e8;
            margin-bottom: 1.5rem;
        }

        .header-text p {
            font-size: 1rem;
            margin-bottom: 1rem;
            max-width: 800px;
        }

        /* Container */
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 2rem;
        }

        /* Section Styling */
        section {
            padding: 3rem 0;
            border-top: 1px solid #e0e0e0;
        }

        section h2 {
            font-size: 1.8rem;
            font-weight: 400;
            margin-bottom: 2rem;
            color: #333;
        }

        /* Video Section */
        .video-wrapper {
            max-width: 900px;
            margin: 2rem 0;
        }

        .video-container {
            position: relative;
            padding-bottom: 56.25%;
            height: 0;
            overflow: hidden;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        }

        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }

        /* Features Grid */
        .features {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2rem;
            margin: 2rem 0;
        }

        .feature-card {
            padding: 1.5rem;
            background: #f8f9fa;
            border-radius: 4px;
            border-left: 3px solid #1a73e8;
        }

        .feature-card h3 {
            font-size: 1.2rem;
            font-weight: 500;
            margin-bottom: 0.8rem;
            color: #333;
        }

        .feature-card p {
            font-size: 0.95rem;
            color: #666;
            line-height: 1.6;
        }

        /* Results Section */
        .results-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 2rem;
            margin: 2rem 0;
        }

        .result-item {
            text-align: center;
        }

        .result-value {
            font-size: 2.5rem;
            font-weight: 300;
            color: #1a73e8;
            margin-bottom: 0.5rem;
        }

        .result-label {
            font-size: 0.95rem;
            color: #666;
        }

        /* Publication Card */
        .publication {
            display: grid;
            grid-template-columns: 200px 1fr;
            gap: 2rem;
            padding: 2rem 0;
            border-bottom: 1px solid #e0e0e0;
        }

        .publication:last-child {
            border-bottom: none;
        }

        .pub-image {
            width: 200px;
            height: 150px;
            background: #f0f0f0;
            border-radius: 4px;
            overflow: hidden;
        }

        .pub-image img {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        .pub-content h3 {
            font-size: 1.3rem;
            font-weight: 500;
            margin-bottom: 0.5rem;
        }

        .pub-content h3 a {
            color: #333;
            text-decoration: none;
        }

        .pub-content h3 a:hover {
            color: #1a73e8;
        }

        .pub-meta {
            font-size: 0.9rem;
            color: #666;
            margin-bottom: 0.8rem;
        }

        .pub-description {
            font-size: 0.95rem;
            color: #555;
            line-height: 1.6;
            margin-bottom: 1rem;
        }

        .pub-links {
            display: flex;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .pub-links a {
            color: #1a73e8;
            text-decoration: none;
            font-size: 0.9rem;
            padding: 0.3rem 0.8rem;
            border: 1px solid #1a73e8;
            border-radius: 3px;
            transition: all 0.2s;
        }

        .pub-links a:hover {
            background: #1a73e8;
            color: white;
        }

        /* Tags */
        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.8rem;
            margin: 2rem 0;
        }

        .tag {
            padding: 0.4rem 1rem;
            background: #f0f0f0;
            color: #555;
            border-radius: 20px;
            font-size: 0.9rem;
            text-decoration: none;
            transition: all 0.2s;
        }

        .tag:hover {
            background: #1a73e8;
            color: white;
        }

        /* Footer */
        footer {
            background: #f8f9fa;
            padding: 2rem 0;
            margin-top: 4rem;
            border-top: 1px solid #e0e0e0;
            text-align: center;
        }

        footer p {
            color: #666;
            font-size: 0.9rem;
        }

        footer a {
            color: #1a73e8;
            text-decoration: none;
        }

        footer a:hover {
            text-decoration: underline;
        }

        /* List Styling */
        ul.tech-list {
            list-style: none;
            margin: 1.5rem 0;
        }

        ul.tech-list li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
        }

        ul.tech-list li:before {
            content: "▸";
            position: absolute;
            left: 0;
            color: #1a73e8;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .header-content {
                grid-template-columns: 1fr;
                text-align: center;
            }

            .profile-image {
                margin: 0 auto;
            }

            .publication {
                grid-template-columns: 1fr;
            }

            .pub-image {
                margin: 0 auto;
            }

            nav ul {
                flex-direction: column;
                gap: 1rem;
            }

            .features {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <!-- Navigation -->
    <nav>
        <div class="container">
            <div class="logo">RL Navigation Project</div>
            <ul>
                <li><a href="#about">About</a></li>
                <li><a href="#demo">Demo</a></li>
                <li><a href="#results">Results</a></li>
                <li><a href="#publications">Publications</a></li>
                <li><a href="#code">Code</a></li>
            </ul>
        </div>
    </nav>

    <!-- Header -->
    <div class="header">
        <div class="header-content">
            <div class="profile-image">RL</div>
            <div class="header-text">
                <h1>Deep Q-Learning for Autonomous Navigation</h1>
                <p class="subtitle">Advanced Reinforcement Learning Research</p>
                <p class="affiliation">Machine Learning Lab • University Research</p>
                <p>
                    This project demonstrates state-of-the-art Deep Q-Learning (DQN) techniques for training autonomous agents 
                    to navigate complex environments. Our approach combines experience replay, double Q-learning, and prioritized 
                    experience replay to achieve robust performance across diverse scenarios.
                </p>
                <p>
                    The agent learns optimal policies through trial and error, maximizing cumulative rewards while developing 
                    sophisticated navigation strategies that generalize to unseen environments.
                </p>
            </div>
        </div>
    </div>

    <!-- About Section -->
    <section id="about">
        <div class="container">
            <h2>Research Overview</h2>
            <p>
                Deep reinforcement learning has emerged as a powerful paradigm for training autonomous agents. This project 
                investigates how DQN algorithms can be applied to complex navigation tasks, where agents must learn to make 
                sequential decisions in partially observable environments.
            </p>
            <p style="margin-top: 1rem;">
                Our research focuses on improving sample efficiency, stability, and generalization capabilities through 
                architectural innovations and training strategies. We evaluate our methods on standard benchmarks and 
                demonstrate significant improvements over baseline approaches.
            </p>

            <div class="features">
                <div class="feature-card">
                    <h3>Deep Q-Network Architecture</h3>
                    <p>
                        Implements DQN with experience replay buffer and target networks for stable training. 
                        Uses convolutional neural networks for visual state representation and double DQN to reduce overestimation bias.
                    </p>
                </div>
                <div class="feature-card">
                    <h3>Prioritized Experience Replay</h3>
                    <p>
                        Samples important transitions more frequently based on TD-error magnitude. 
                        Accelerates learning by focusing on experiences with highest learning potential.
                    </p>
                </div>
                <div class="feature-card">
                    <h3>Robust Generalization</h3>
                    <p>
                        Trained agents demonstrate strong transfer learning capabilities. Successfully 
                        navigates novel environments with different layouts and obstacle configurations.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <!-- Demo Video Section -->
    <section id="demo">
        <div class="container">
            <h2>Agent Demonstration</h2>
            <p>
                Watch our trained agent navigate through a complex maze environment. The agent has learned to avoid 
                obstacles, plan efficient paths, and adapt to dynamic conditions through pure reinforcement learning.
            </p>
            
            <div class="video-wrapper">
                <div class="video-container">
                    <!-- Replace VIDEO_ID with your YouTube video ID -->
                    <iframe 
                        src="https://www.youtube.com/watch?v=XteDpNsX75A" 
                        frameborder="0" 
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                        allowfullscreen>
                    </iframe>
                </div>
            </div>

            <p style="margin-top: 1.5rem;">
                The video demonstrates the agent's behavior after 1M training steps. Notice how it efficiently navigates 
                around obstacles and finds near-optimal paths to the goal, showcasing emergent intelligent behavior 
                learned purely from reward signals.
            </p>
        </div>
    </section>

    <!-- Results Section -->
    <section id="results">
        <div class="container">
            <h2>Experimental Results</h2>
            <p>
                Our experiments demonstrate significant improvements over baseline DQN implementations. The agent achieves 
                superhuman performance on standard benchmarks and exhibits strong generalization to novel scenarios.
            </p>

            <div class="results-grid">
                <div class="result-item">
                    <div class="result-value">95%</div>
                    <div class="result-label">Success Rate</div>
                </div>
                <div class="result-item">
                    <div class="result-value">250+</div>
                    <div class="result-label">Average Reward</div>
                </div>
                <div class="result-item">
                    <div class="result-value">1M</div>
                    <div class="result-label">Training Steps</div>
                </div>
                <div class="result-item">
                    <div class="result-value">40%</div>
                    <div class="result-label">Efficiency Gain</div>
                </div>
            </div>

            <h3 style="margin-top: 3rem; margin-bottom: 1rem; font-weight: 400; font-size: 1.3rem;">Key Findings</h3>
            <ul class="tech-list">
                <li>Prioritized experience replay reduces training time by 40% compared to uniform sampling</li>
                <li>Double DQN eliminates overestimation bias, leading to more stable policy learning</li>
                <li>Agents trained with curriculum learning show 30% better generalization to novel environments</li>
                <li>Frame stacking and reward clipping improve robustness across diverse task variations</li>
            </ul>
        </div>
    </section>

    <!-- Technical Details -->
    <section id="technical">
        <div class="container">
            <h2>Technical Specifications</h2>
            
            <h3 style="margin-bottom: 1rem; font-weight: 400; font-size: 1.3rem;">Algorithm</h3>
            <p>
                We implement a variant of DQN that incorporates several key improvements from the deep RL literature. 
                The core algorithm uses a convolutional neural network to approximate the action-value function, with 
                separate target and online networks to stabilize training.
            </p>

            <ul class="tech-list" style="margin-top: 1rem;">
                <li><strong>Base Algorithm:</strong> Deep Q-Learning with Double DQN improvements</li>
                <li><strong>Experience Replay:</strong> Prioritized replay buffer with 1M transitions</li>
                <li><strong>Network Architecture:</strong> 3-layer CNN + 2-layer MLP (512 hidden units)</li>
                <li><strong>Framework:</strong> PyTorch 2.0 with CUDA acceleration</li>
                <li><strong>Environment:</strong> OpenAI Gym / Custom navigation environments</li>
                <li><strong>Training Details:</strong> 1M timesteps, ε-greedy exploration (1.0 → 0.1 over 100k steps)</li>
                <li><strong>Hyperparameters:</strong> Learning rate 0.0001, γ=0.99, batch size 32, target network update every 1000 steps</li>
            </ul>

            <h3 style="margin-top: 2rem; margin-bottom: 1rem; font-weight: 400; font-size: 1.3rem;">Research Topics</h3>
            <div class="tags">
                <span class="tag">Deep Reinforcement Learning</span>
                <span class="tag">Q-Learning</span>
                <span class="tag">Neural Networks</span>
                <span class="tag">Autonomous Navigation</span>
                <span class="tag">Experience Replay</span>
                <span class="tag">Generalization</span>
                <span class="tag">Sample Efficiency</span>
                <span class="tag">Computer Vision</span>
            </div>
        </div>
    </section>

    <!-- Publications Section -->
    <section id="publications">
        <div class="container">
            <h2>Related Publications</h2>

            <div class="publication">
                <div class="pub-image">
                    <img src="https://via.placeholder.com/200x150/667eea/ffffff?text=DQN" alt="Publication thumbnail">
                </div>
                <div class="pub-content">
                    <h3><a href="#">Deep Q-Learning for Robust Autonomous Navigation</a></h3>
                    <p class="pub-meta">Your Name, Co-Author • 2024 • International Conference on Machine Learning (ICML)</p>
                    <p class="pub-description">
                        We present a novel approach to training navigation agents using Deep Q-Learning with prioritized 
                        experience replay. Our method achieves state-of-the-art performance on standard benchmarks while 
                        demonstrating superior generalization to novel environments.
                    </p>
                    <div class="pub-links">
                        <a href="#">PDF</a>
                        <a href="#">Code</a>
                        <a href="#">BibTeX</a>
                        <a href="#">Slides</a>
                    </div>
                </div>
            </div>

            <div class="publication">
                <div class="pub-image">
                    <img src="https://via.placeholder.com/200x150/764ba2/ffffff?text=Transfer" alt="Publication thumbnail">
                </div>
                <div class="pub-content">
                    <h3><a href="#">Transfer Learning in Deep Reinforcement Learning for Navigation</a></h3>
                    <p class="pub-meta">Your Name, Co-Author • 2023 • Neural Information Processing Systems (NeurIPS) Workshop</p>
                    <p class="pub-description">
                        This work investigates how navigation policies learned in simulation can be effectively transferred 
                        to real-world robotic platforms. We develop techniques for domain randomization and sim-to-real 
                        transfer that significantly reduce the reality gap.
                    </p>
                    <div class="pub-links">
                        <a href="#">PDF</a>
                        <a href="#">Poster</a>
                        <a href="#">Video</a>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Code Section -->
    <section id="code">
        <div class="container">
            <h2>Code & Resources</h2>
            <p>
                All code for this project is open-source and available on GitHub. We provide implementations of the core 
                algorithms, pre-trained models, and utilities for reproducing our experimental results.
            </p>

            <div class="features" style="margin-top: 2rem;">
                <div class="feature-card">
                    <h3>GitHub Repository</h3>
                    <p>
                        Complete implementation with training scripts, evaluation tools, and documentation. 
                        <a href="https://github.com/yourusername/dqn-navigation" style="color: #1a73e8;">View on GitHub →</a>
                    </p>
                </div>
                <div class="feature-card">
                    <h3>Pre-trained Models</h3>
                    <p>
                        Download trained model checkpoints to reproduce results or use as starting points for your research.
                        <a href="#" style="color: #1a73e8;">Download models →</a>
                    </p>
                </div>
                <div class="feature-card">
                    <h3>Documentation</h3>
                    <p>
                        Comprehensive guides for installation, training, and evaluation. Includes tutorials and example notebooks.
                        <a href="#" style="color: #1a73e8;">Read docs →</a>
                    </p>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer>
        <div class="container">
            <p>
                <a href="https://github.com/yourusername">GitHub</a> • 
                <a href="mailto:your.email@university.edu">Contact</a> • 
                <a href="#">Google Scholar</a> • 
                <a href="#">Twitter</a>
            </p>
            <p style="margin-top: 1rem;">&copy; 2026 Your Name. Built with HTML & CSS.</p>
        </div>
    </footer>
</body>
</html>
